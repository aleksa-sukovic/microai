{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is All You Need\n",
    "\n",
    "This notebook is an implementation of a transformer model introduced in the paper \"Attention is all you need\" [1]. The model is trained on a small dataset of Pink Floyd lyrics [2].\n",
    "\n",
    "### References\n",
    "\n",
    "1. A. Vaswani et al., “Attention Is All You Need.” arXiv, Dec. 05, 2017. doi: 10.48550/arXiv.1706.03762.\n",
    "2. J. Robson, \"Pink Floyd Lyrics\", retrieved from [url](https://www.kaggle.com/datasets/joaorobson/pink-floyd-lyrics/code).\n",
    "3. R. Sennrich, B. Haddow, A. Birch, \"Neural Machine Translation of Rare WOrds with Subword Units\", 2016. doi: 10.48550/arXiv.1508.07909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Tokenization\n",
    "\n",
    "We start by preprocessing the dataset and training a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>song_title</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Piper at the Gates of Dawn</td>\n",
       "      <td>Astronomy Domine</td>\n",
       "      <td>1967-08-05</td>\n",
       "      <td>\"Moon in both [houses]...\"...Scorpio, [Arabian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Piper at the Gates of Dawn</td>\n",
       "      <td>Lucifer Sam</td>\n",
       "      <td>1967-08-05</td>\n",
       "      <td>Lucifer Sam, siam cat\\nAlways sitting by your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Piper at the Gates of Dawn</td>\n",
       "      <td>Matilda Mother</td>\n",
       "      <td>1967-08-05</td>\n",
       "      <td>There was a king who ruled the land\\nHis Majes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Piper at the Gates of Dawn</td>\n",
       "      <td>Flaming</td>\n",
       "      <td>1967-08-05</td>\n",
       "      <td>Alone in the clouds all blue\\nLying on an eide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Piper at the Gates of Dawn</td>\n",
       "      <td>Pow R. Toc H.</td>\n",
       "      <td>1967-08-05</td>\n",
       "      <td>TCH TCH\\nAHH (AHH)\\nTCH TCH\\nAHH AHH\\nDoi doi\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            album        song_title        year  \\\n",
       "0  The Piper at the Gates of Dawn  Astronomy Domine  1967-08-05   \n",
       "1  The Piper at the Gates of Dawn       Lucifer Sam  1967-08-05   \n",
       "2  The Piper at the Gates of Dawn    Matilda Mother  1967-08-05   \n",
       "3  The Piper at the Gates of Dawn           Flaming  1967-08-05   \n",
       "4  The Piper at the Gates of Dawn     Pow R. Toc H.  1967-08-05   \n",
       "\n",
       "                                              lyrics  \n",
       "0  \"Moon in both [houses]...\"...Scorpio, [Arabian...  \n",
       "1  Lucifer Sam, siam cat\\nAlways sitting by your ...  \n",
       "2  There was a king who ruled the land\\nHis Majes...  \n",
       "3  Alone in the clouds all blue\\nLying on an eide...  \n",
       "4  TCH TCH\\nAHH (AHH)\\nTCH TCH\\nAHH AHH\\nDoi doi\\...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./assets/data/pink_floyd_lyrics.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data[\"album\"].isin([\"The Piper at the Gates of Dawn\", \"A Saucerful of Secrets\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Cleaning\n",
    "\n",
    "This version of the dataset is quite noisy and contains unformatted lyrics (see e.g. [Pink Floyd dataset of Huggingface](https://huggingface.co/datasets/huggingartists/pink-floyd) that is in a better state). Nevertheless, to demonstrate somewhat of a realistic data preprocessing flow, we will stick to this particular version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(columns=[\"album\", \"song_title\", \"year\"])\n",
    "df = df.dropna()\n",
    "\n",
    "df = df.replace(\"\\((.*?)\\),? ?\", \"\", regex=True)   # remove round brackets and content\n",
    "df = df.replace(\"\\[(.*?)\\],? ?\", \"\", regex=True)   # remove round brackets and content\n",
    "df = df.replace(\"[\\\"“”…]\", \"\", regex=True)         # remove \"\n",
    "df = df.replace(\"\\.{3,}\", \"...\", regex=True)       # replace multiple dots with three dots\n",
    "df = df.replace(\"(\\*.*\\*)\", \"\", regex=True)        # remove sound effects between *\n",
    "df = df.replace(\"[\\:\\-\\.\\!\\?]\", \" \", regex=True)   # remove :, -, ., !, ?\n",
    "df = df.replace(\"\\\\\\\\ n\", \"\\n\", regex=True)        # remove ill-formatted newlines\n",
    "df = df.replace(\"\\\\\\\\\", \"\", regex=True)            # remove \\\n",
    "df = df.replace(\"(\\\\n)+\", \"\\\\n\", regex=True) # remove multiple newlines\n",
    "df = df.replace(\" +\", \" \", regex=True)             # remove multiple spaces\n",
    "df = df.replace(\"\\n \", \"\\n\", regex=True)           # remove leading spaces after newline\n",
    "\n",
    "df[\"lyrics\"] = df[\"lyrics\"].str.lower()            # lowercase\n",
    "df[\"lyrics\"] = df[\"lyrics\"].str.strip(\"-. \")       # remove leading and trailing spaces\n",
    "df[\"lyrics\"] = df[\"lyrics\"].str.replace(\"\\\\n\", \" \", regex=True)\n",
    "\n",
    "lyrics = [l for l in df.lyrics]\n",
    "lyrics = \"\\n\".join(lyrics)                      # add EOS token between songs\n",
    "\n",
    "with open(\"./assets/data/pink_floyd_lyrics.txt\", \"w\") as f: f.write(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Next up, we tokenize the obtained sentences. Following the original paper, we will utilize byte-pair encoding [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFD, Lowercase, Strip, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents(), Lowercase(), Strip()])\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 4102\n"
     ]
    }
   ],
   "source": [
    "trainer = BpeTrainer(special_tokens=[\"[BOS]\"], show_progress=False)\n",
    "tokenizer.train_from_iterator([lyrics], trainer=trainer)\n",
    "print(f\"Vocabulary Size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this section, we train a decoder-only transformer model to predict the next word using our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microai.models.transformer import TransformerConfig, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=64,\n",
    "    num_heads=8,\n",
    "    context_size=128,\n",
    "    dropout=0.2,\n",
    "    decoder_layers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "batch_size = 32\n",
    "epochs = 2000\n",
    "eval_freq = 50\n",
    "weight_decay = 1e-2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {k: v for k, v in model.named_parameters() if v.requires_grad}\n",
    "\n",
    "params_decay = [v for _, v in params.items() if v.dim() >= 2]\n",
    "params_no_decay = [v for _, v in params.items() if v.dim() < 2]\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": params_decay, \"weight_decay\": weight_decay},\n",
    "    {\"params\": params_no_decay, \"weight_decay\": 0.0}\n",
    "], lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(tokens: List[List[int]], batch_size: int):\n",
    "    def _pad(sequence: List[int], size: int):\n",
    "        return [tokenizer.token_to_id(\"[BOS]\")] * (size - len(sequence)) + sequence\n",
    "    \n",
    "    ids = torch.randperm(len(tokens))[:batch_size].tolist()\n",
    "    input = [tokens[i] for i in range(len(tokens)) if i in ids]\n",
    "    max_size = max([len(i) for i in input])\n",
    "    input = [_pad(i, max_size) for i in input]\n",
    "    input = torch.tensor(input)\n",
    "\n",
    "    return input[:, :-1], input[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(tokens: List[List[int]], batch_size: int = 5, num_batches: int = 25):\n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        x, y = get_batch(tokens, batch_size=batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred.view((-1, y_pred.size(-1))), y.view(-1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data: pd.DataFrame, context_size: int):\n",
    "    items = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        tokens = tokenizer.encode(row[\"lyrics\"]).ids\n",
    "        for batch in range(len(tokens) // context_size + 1):\n",
    "            item_tokens = tokens[batch * context_size: (batch + 1) * context_size]\n",
    "            items.append(item_tokens)\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_data(df, config.context_size)\n",
    "\n",
    "train_chunk = 0.9\n",
    "train_ids = torch.randperm(len(tokens))[:int(len(tokens) * train_chunk)].tolist()\n",
    "\n",
    "train_tokens = [tokens[i] for i in range(len(tokens)) if i in train_ids]\n",
    "test_tokens = [tokens[i] for i in range(len(tokens)) if i not in train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 10.188, Test Loss: 9.912\n",
      "Epoch: 50, Train Loss: 5.298, Test Loss: 4.907\n",
      "Epoch: 100, Train Loss: 5.022, Test Loss: 4.531\n",
      "Epoch: 150, Train Loss: 4.546, Test Loss: 4.525\n",
      "Epoch: 200, Train Loss: 4.376, Test Loss: 3.997\n",
      "Epoch: 250, Train Loss: 4.408, Test Loss: 4.140\n",
      "Epoch: 300, Train Loss: 3.971, Test Loss: 3.803\n",
      "Epoch: 350, Train Loss: 3.975, Test Loss: 3.813\n",
      "Epoch: 400, Train Loss: 4.006, Test Loss: 3.732\n",
      "Epoch: 450, Train Loss: 3.625, Test Loss: 3.910\n",
      "Epoch: 500, Train Loss: 3.893, Test Loss: 3.877\n",
      "Epoch: 550, Train Loss: 3.605, Test Loss: 3.954\n",
      "Epoch: 600, Train Loss: 3.722, Test Loss: 3.903\n",
      "Epoch: 650, Train Loss: 3.586, Test Loss: 3.800\n",
      "Epoch: 700, Train Loss: 3.834, Test Loss: 3.810\n",
      "Epoch: 750, Train Loss: 3.367, Test Loss: 3.959\n",
      "Epoch: 800, Train Loss: 3.513, Test Loss: 3.693\n",
      "Epoch: 850, Train Loss: 3.435, Test Loss: 4.001\n",
      "Epoch: 900, Train Loss: 3.452, Test Loss: 3.803\n",
      "Epoch: 950, Train Loss: 3.655, Test Loss: 3.967\n",
      "Epoch: 1000, Train Loss: 3.514, Test Loss: 3.845\n",
      "Epoch: 1050, Train Loss: 3.820, Test Loss: 3.812\n",
      "Epoch: 1100, Train Loss: 3.326, Test Loss: 3.905\n",
      "Epoch: 1150, Train Loss: 3.403, Test Loss: 3.915\n",
      "Epoch: 1200, Train Loss: 3.573, Test Loss: 3.853\n",
      "Epoch: 1250, Train Loss: 3.346, Test Loss: 3.825\n",
      "Epoch: 1300, Train Loss: 3.641, Test Loss: 3.783\n",
      "Epoch: 1350, Train Loss: 3.535, Test Loss: 3.981\n",
      "Epoch: 1400, Train Loss: 3.695, Test Loss: 3.882\n",
      "Epoch: 1450, Train Loss: 3.764, Test Loss: 3.723\n",
      "Epoch: 1500, Train Loss: 3.491, Test Loss: 4.014\n",
      "Epoch: 1550, Train Loss: 3.505, Test Loss: 3.498\n",
      "Epoch: 1600, Train Loss: 3.554, Test Loss: 3.700\n",
      "Epoch: 1650, Train Loss: 3.540, Test Loss: 3.729\n",
      "Epoch: 1700, Train Loss: 3.444, Test Loss: 3.952\n",
      "Epoch: 1750, Train Loss: 3.554, Test Loss: 3.812\n",
      "Epoch: 1800, Train Loss: 3.264, Test Loss: 3.741\n",
      "Epoch: 1850, Train Loss: 3.644, Test Loss: 3.544\n",
      "Epoch: 1900, Train Loss: 3.423, Test Loss: 3.914\n",
      "Epoch: 1950, Train Loss: 3.385, Test Loss: 3.665\n",
      "Epoch: 2000, Train Loss: 3.548, Test Loss: 3.949\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    x, y = get_batch(train_tokens, batch_size=batch_size)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = F.cross_entropy(y_pred.view((-1, y_pred.size(-1))), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % eval_freq == 0 or epoch == 1:\n",
    "        train_loss, test_loss = estimate_loss(train_tokens), estimate_loss(test_tokens)\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {train_loss:.3f}, Test Loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: Transformer, prompt: str, context_size: int = 8, max_length: int = 1000):\n",
    "    context = torch.tensor(tokenizer.encode(prompt).ids, device=device)\n",
    "    model.eval()\n",
    "\n",
    "    while True:    \n",
    "        logits = model(context[-context_size:].unsqueeze(0))\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        token = torch.multinomial(probs[:, -1, :].flatten(), num_samples=1).item()\n",
    "        if context.size(0) >= max_length:\n",
    "            break\n",
    "        context = torch.cat((context, torch.tensor([token], device=device)), dim=0)\n",
    "\n",
    "    model.train()\n",
    "    return tokenizer.decode(context.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shine on ground to make the weak in the animals become the lived now at\n",
      "time has the same we lie out is who for all anced ’ s are\n",
      "money you want and high is in you feel narrow hey you ’ ll to\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"shine on \", context_size=config.context_size, max_length=15))\n",
    "print(generate(model, \"time \", context_size=config.context_size, max_length=15))\n",
    "print(generate(model, \"money \", context_size=config.context_size, max_length=15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
