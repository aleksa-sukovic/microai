{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Probabilistic Language Model\n",
    "\n",
    "This notebook implements a simple neural probabilistic language model [1] using PyTorch. The model is trained on 10k song titles from the Million Song Dataset [2] to predict the next word, given a fixed context window of three preceding words.\n",
    "\n",
    "### References\n",
    "\n",
    "1. Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A Neural Probabilistic Language Model,” Advances in neural information processing systems, vol. 13, 2000.\n",
    "2. Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere.  The Million Song Dataset. In Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2011), 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use a small sample of 10k songs from the [Million Song Dataset](http://millionsongdataset.com/pages/getting-dataset/). In particular, we use only song titles and build a language model that can generate new, similar sounding titles. You can read more about the structure of the dataset [here](http://millionsongdataset.com/pages/example-track-description/), and there are also some [useful code snippets](https://github.com/tbertinmahieux/MSongsDB/tree/master/PythonSrc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tables\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import StripAccents, Lowercase, NFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_titles(dataset_path, limit=None):\n",
    "    result = []\n",
    "\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for file_name in files:\n",
    "            file = tables.open_file(os.path.join(root, file_name))\n",
    "            result += [f.decode(\"utf-8\") for f in file.root.metadata.songs.cols.title]\n",
    "            file.close()\n",
    "            if limit and len(result) >= limit: break\n",
    "    result = result if not limit else result[:limit]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/mnt/storage/Development/Data/million_songs/million_songs_10k\" # change to your dataset path\n",
    "songs = get_song_titles(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "For this example, we will use a simple word-level tokenizer, applying few basic data cleaning transformations. We leave more advance tokenizer architectures for future tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WordLevelTrainer()\n",
    "tokenizer.train_from_iterator(songs, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "We collect few of the configuration variables pertaining to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6687945581\n",
    "window_size = 3\n",
    "train_chunk = 0.9\n",
    "val_chunk = 0.1\n",
    "vocab_size = tokenizer.get_vocab_size() + 1\n",
    "empty_token_id = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We create a torch dataset comprising of the context words, number of which is specified by parameter `window_size`, and associated targets, representing the next word from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(songs: List[str], window_size: int = 3) -> torch.utils.data.Dataset:\n",
    "    x, y = [], []\n",
    "\n",
    "    songs_encoded = tokenizer.encode_batch(songs)\n",
    "    songs_encoded = [song.ids + [empty_token_id] for song in songs_encoded]\n",
    "\n",
    "    for song in songs_encoded:\n",
    "        window = [empty_token_id] * window_size\n",
    "        for token in song:\n",
    "            x += [window]\n",
    "            y += [token]\n",
    "            window = window[1:] + [token]\n",
    "\n",
    "    x, y = torch.tensor(x), F.one_hot(torch.tensor(y), vocab_size).float()\n",
    "    return torch.utils.data.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(songs)\n",
    "train_limit, val_limit = int(len(songs) * train_chunk), int(len(songs) * (train_chunk + val_chunk))\n",
    "train_dataset = get_dataset(songs[:train_limit], window_size=window_size)\n",
    "val_dataset = get_dataset(songs[train_limit:val_limit], window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9805\n",
      "Train dataset size: 47879\n",
      "Val dataset size: 5370\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In this section, we build a simple neural network to predict a subsequent word given context. The architecture of the network is described in the original paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, window_size: int, residual: bool = False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.residual = residual\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.C = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.H = nn.Linear(embedding_dim * window_size, hidden_dim)\n",
    "        self.U = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.W = nn.Linear(embedding_dim * window_size, vocab_size) if residual else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.C(x).view(-1, self.embedding_dim * self.window_size)\n",
    "        embeddings = torch.tanh(self.H(x))\n",
    "        return self.W(x) + self.U(embeddings) if self.residual else self.U(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Next, we implement the network training and evaluation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NPLanguageModel(vocab_size, embedding_dim=32, hidden_dim=128, residual=True, window_size=window_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 75, 150], gamma=0.1)\n",
    "epochs = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train():\n",
    "    losses = []\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return torch.tensor(losses).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def epoch_eval(dataloader):\n",
    "    losses = []\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return torch.tensor(losses).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1; train_loss=6.64286\n",
      "epoch=2; train_loss=5.86656\n",
      "epoch=3; train_loss=5.64640\n",
      "epoch=4; train_loss=5.50319\n",
      "epoch=5; train_loss=5.38981\n",
      "epoch=6; train_loss=5.29672\n",
      "epoch=7; train_loss=5.21287\n",
      "epoch=8; train_loss=5.13335\n",
      "epoch=9; train_loss=5.06213\n",
      "epoch=10; train_loss=4.98731\n",
      "epoch=10; val_loss=4.98731\n",
      "epoch=11; train_loss=4.92671\n",
      "epoch=12; train_loss=4.86029\n",
      "epoch=13; train_loss=4.80187\n",
      "epoch=14; train_loss=4.74224\n",
      "epoch=15; train_loss=4.69029\n",
      "epoch=16; train_loss=4.63780\n",
      "epoch=17; train_loss=4.58716\n",
      "epoch=18; train_loss=4.53418\n",
      "epoch=19; train_loss=4.48587\n",
      "epoch=20; train_loss=4.43955\n",
      "epoch=20; val_loss=4.43955\n",
      "epoch=21; train_loss=4.39750\n",
      "epoch=22; train_loss=4.34973\n",
      "epoch=23; train_loss=4.30722\n",
      "epoch=24; train_loss=4.26751\n",
      "epoch=25; train_loss=4.22838\n",
      "epoch=26; train_loss=4.19013\n",
      "epoch=27; train_loss=4.15285\n",
      "epoch=28; train_loss=4.11508\n",
      "epoch=29; train_loss=4.08168\n",
      "epoch=30; train_loss=4.04791\n",
      "epoch=30; val_loss=4.04791\n",
      "epoch=31; train_loss=4.01612\n",
      "epoch=32; train_loss=3.98418\n",
      "epoch=33; train_loss=3.95263\n",
      "epoch=34; train_loss=3.92089\n",
      "epoch=35; train_loss=3.88889\n",
      "epoch=36; train_loss=3.86376\n",
      "epoch=37; train_loss=3.83518\n",
      "epoch=38; train_loss=3.80853\n",
      "epoch=39; train_loss=3.78286\n",
      "epoch=40; train_loss=3.75328\n",
      "epoch=40; val_loss=3.75328\n",
      "epoch=41; train_loss=3.73023\n",
      "epoch=42; train_loss=3.70478\n",
      "epoch=43; train_loss=3.67947\n",
      "epoch=44; train_loss=3.65701\n",
      "epoch=45; train_loss=3.63379\n",
      "epoch=46; train_loss=3.61249\n",
      "epoch=47; train_loss=3.59143\n",
      "epoch=48; train_loss=3.57243\n",
      "epoch=49; train_loss=3.55180\n",
      "epoch=50; train_loss=3.52643\n",
      "epoch=50; val_loss=3.52643\n",
      "epoch=51; train_loss=3.50725\n",
      "epoch=52; train_loss=3.48948\n",
      "epoch=53; train_loss=3.46905\n",
      "epoch=54; train_loss=3.45184\n",
      "epoch=55; train_loss=3.43283\n",
      "epoch=56; train_loss=3.41432\n",
      "epoch=57; train_loss=3.39690\n",
      "epoch=58; train_loss=3.38119\n",
      "epoch=59; train_loss=3.36410\n",
      "epoch=60; train_loss=3.34950\n",
      "epoch=60; val_loss=3.34950\n",
      "epoch=61; train_loss=3.33264\n",
      "epoch=62; train_loss=3.31734\n",
      "epoch=63; train_loss=3.29836\n",
      "epoch=64; train_loss=3.28485\n",
      "epoch=65; train_loss=3.27196\n",
      "epoch=66; train_loss=3.25618\n",
      "epoch=67; train_loss=3.23960\n",
      "epoch=68; train_loss=3.22390\n",
      "epoch=69; train_loss=3.21096\n",
      "epoch=70; train_loss=3.19835\n",
      "epoch=70; val_loss=3.19835\n",
      "epoch=71; train_loss=3.18417\n",
      "epoch=72; train_loss=3.17211\n",
      "epoch=73; train_loss=3.15721\n",
      "epoch=74; train_loss=3.14376\n",
      "epoch=75; train_loss=3.13524\n",
      "epoch=76; train_loss=3.11789\n",
      "epoch=77; train_loss=3.10728\n",
      "epoch=78; train_loss=3.09228\n",
      "epoch=79; train_loss=3.08145\n",
      "epoch=80; train_loss=3.06942\n",
      "epoch=80; val_loss=3.06942\n",
      "epoch=81; train_loss=3.05937\n",
      "epoch=82; train_loss=3.04544\n",
      "epoch=83; train_loss=3.03489\n",
      "epoch=84; train_loss=3.02416\n",
      "epoch=85; train_loss=3.01378\n",
      "epoch=86; train_loss=3.00387\n",
      "epoch=87; train_loss=2.99148\n",
      "epoch=88; train_loss=2.98090\n",
      "epoch=89; train_loss=2.97180\n",
      "epoch=90; train_loss=2.96408\n",
      "epoch=90; val_loss=2.96408\n",
      "epoch=91; train_loss=2.95046\n",
      "epoch=92; train_loss=2.94209\n",
      "epoch=93; train_loss=2.93464\n",
      "epoch=94; train_loss=2.92180\n",
      "epoch=95; train_loss=2.91355\n",
      "epoch=96; train_loss=2.90257\n",
      "epoch=97; train_loss=2.89328\n",
      "epoch=98; train_loss=2.88362\n",
      "epoch=99; train_loss=2.87960\n",
      "epoch=100; train_loss=2.86918\n",
      "epoch=100; val_loss=2.86918\n",
      "epoch=101; train_loss=2.86013\n",
      "epoch=102; train_loss=2.85090\n",
      "epoch=103; train_loss=2.84639\n",
      "epoch=104; train_loss=2.83468\n",
      "epoch=105; train_loss=2.82711\n",
      "epoch=106; train_loss=2.81944\n",
      "epoch=107; train_loss=2.81326\n",
      "epoch=108; train_loss=2.80765\n",
      "epoch=109; train_loss=2.80083\n",
      "epoch=110; train_loss=2.79210\n",
      "epoch=110; val_loss=2.79210\n",
      "epoch=111; train_loss=2.78630\n",
      "epoch=112; train_loss=2.77685\n",
      "epoch=113; train_loss=2.77131\n",
      "epoch=114; train_loss=2.76415\n",
      "epoch=115; train_loss=2.76120\n",
      "epoch=116; train_loss=2.75453\n",
      "epoch=117; train_loss=2.74579\n",
      "epoch=118; train_loss=2.74289\n",
      "epoch=119; train_loss=2.73499\n",
      "epoch=120; train_loss=2.72855\n",
      "epoch=120; val_loss=2.72855\n",
      "epoch=121; train_loss=2.72237\n",
      "epoch=122; train_loss=2.72049\n",
      "epoch=123; train_loss=2.70992\n",
      "epoch=124; train_loss=2.70704\n",
      "epoch=125; train_loss=2.70213\n",
      "epoch=126; train_loss=2.69615\n",
      "epoch=127; train_loss=2.69133\n",
      "epoch=128; train_loss=2.68628\n",
      "epoch=129; train_loss=2.68148\n",
      "epoch=130; train_loss=2.67438\n",
      "epoch=130; val_loss=2.67438\n",
      "epoch=131; train_loss=2.66962\n",
      "epoch=132; train_loss=2.66706\n",
      "epoch=133; train_loss=2.66105\n",
      "epoch=134; train_loss=2.65478\n",
      "epoch=135; train_loss=2.65190\n",
      "epoch=136; train_loss=2.64676\n",
      "epoch=137; train_loss=2.64441\n",
      "epoch=138; train_loss=2.63754\n",
      "epoch=139; train_loss=2.63732\n",
      "epoch=140; train_loss=2.62929\n",
      "epoch=140; val_loss=2.62929\n",
      "epoch=141; train_loss=2.62446\n",
      "epoch=142; train_loss=2.62310\n",
      "epoch=143; train_loss=2.61784\n",
      "epoch=144; train_loss=2.61588\n",
      "epoch=145; train_loss=2.60886\n",
      "epoch=146; train_loss=2.60772\n",
      "epoch=147; train_loss=2.60452\n",
      "epoch=148; train_loss=2.59784\n",
      "epoch=149; train_loss=2.59257\n",
      "epoch=150; train_loss=2.58940\n",
      "epoch=150; val_loss=2.58940\n",
      "epoch=151; train_loss=2.58590\n",
      "epoch=152; train_loss=2.58192\n",
      "epoch=153; train_loss=2.57691\n",
      "epoch=154; train_loss=2.57534\n",
      "epoch=155; train_loss=2.57232\n",
      "epoch=156; train_loss=2.56785\n",
      "epoch=157; train_loss=2.56537\n",
      "epoch=158; train_loss=2.55975\n",
      "epoch=159; train_loss=2.55996\n",
      "epoch=160; train_loss=2.55439\n",
      "epoch=160; val_loss=2.55439\n",
      "epoch=161; train_loss=2.55433\n",
      "epoch=162; train_loss=2.54853\n",
      "epoch=163; train_loss=2.54738\n",
      "epoch=164; train_loss=2.54235\n",
      "epoch=165; train_loss=2.53853\n",
      "epoch=166; train_loss=2.53596\n",
      "epoch=167; train_loss=2.53307\n",
      "epoch=168; train_loss=2.52952\n",
      "epoch=169; train_loss=2.52743\n",
      "epoch=170; train_loss=2.52548\n",
      "epoch=170; val_loss=2.52548\n",
      "epoch=171; train_loss=2.51891\n",
      "epoch=172; train_loss=2.51922\n",
      "epoch=173; train_loss=2.51788\n",
      "epoch=174; train_loss=2.51372\n",
      "epoch=175; train_loss=2.50956\n",
      "epoch=176; train_loss=2.50891\n",
      "epoch=177; train_loss=2.50458\n",
      "epoch=178; train_loss=2.50298\n",
      "epoch=179; train_loss=2.49898\n",
      "epoch=180; train_loss=2.49672\n",
      "epoch=180; val_loss=2.49672\n",
      "epoch=181; train_loss=2.49220\n",
      "epoch=182; train_loss=2.49268\n",
      "epoch=183; train_loss=2.48802\n",
      "epoch=184; train_loss=2.48992\n",
      "epoch=185; train_loss=2.48372\n",
      "epoch=186; train_loss=2.48474\n",
      "epoch=187; train_loss=2.47964\n",
      "epoch=188; train_loss=2.47701\n",
      "epoch=189; train_loss=2.47384\n",
      "epoch=190; train_loss=2.47352\n",
      "epoch=190; val_loss=2.47352\n",
      "epoch=191; train_loss=2.46973\n",
      "epoch=192; train_loss=2.46769\n",
      "epoch=193; train_loss=2.46668\n",
      "epoch=194; train_loss=2.46285\n",
      "epoch=195; train_loss=2.46342\n",
      "epoch=196; train_loss=2.45900\n",
      "epoch=197; train_loss=2.45756\n",
      "epoch=198; train_loss=2.45483\n",
      "epoch=199; train_loss=2.45179\n",
      "epoch=200; train_loss=2.45214\n",
      "epoch=200; val_loss=2.45214\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = epoch_train()\n",
    "    print(f\"epoch={epoch}; train_loss={train_loss:.5f}\")\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        val_loss = epoch_eval(val_loader)\n",
    "        print(f\"epoch={epoch}; val_loss={train_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "In the end, we sample few song titles using our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "num_titles = 10\n",
    "generator = torch.Generator(device).manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firehouse old la\n",
      "i want an angel\n",
      "price never to be lonely\n",
      "reminisce look dial you\n",
      "stand down\n",
      "the river\n",
      "broke train\n",
      "medley : fred\n",
      "traveling air i dream\n",
      "working on the head\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_titles):\n",
    "    tokens = []\n",
    "    window = [empty_token_id] * window_size\n",
    "\n",
    "    while True:\n",
    "        window = torch.tensor([window]).to(device)\n",
    "\n",
    "        logits = model(window)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        token = torch.multinomial(probs, num_samples=1, generator=generator).item()\n",
    "\n",
    "        if token == empty_token_id: break\n",
    "        tokens.append(token)\n",
    "        window = window.flatten().tolist()[1:] + [token]\n",
    "\n",
    "    print(tokenizer.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
